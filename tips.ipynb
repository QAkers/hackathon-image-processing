{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogramas\n",
    "\n",
    "En esta sección vamos a conocer los histogramas, y ver como pueden servirnos de cara a la mejora de contraste de una imagen, así como una primera clasificación de objetos dentro de una imagen.\n",
    "\n",
    "Pero primero, ¿que es un histograma?, es una representación gráfica de la distribución de los distintos tonos de una imagen. Puede ayudarnos para controlar la exposición en nuestras fotos, así como para corregir los colores.\n",
    "\n",
    "### cv2.calcHist(images, channels, mask, histSize, ranges)\n",
    "* images : imagen fuente en formato uint8 o float32. Se debe definir entre corchetes, ie, “[img]”.\n",
    "* channels :Se define también entre corchetes. Es el indice del canal sobre cual calcularemos el histograma. Por ejemplo, si la entrada es una imagen en escala de grises, el valor será [0]. Para una imagen a color, puedes definir [0], [1] o [2] para calcular el histograma de azul ( R ), verde ( G ), o rojo ( R ) respectivamente.\n",
    "* mask : Imagen máscara. Si pretendes realizar el histograma de toda una imagen, debes setear el valor a ‘None’. Pero si tu quieres realizar el histograma de una región en particular, deberás definir esa región a traves de esta imagen mascara.\n",
    "* histSize : representar nuestro contador BIN. Se define entre corchetes []. La escala masiva que se le puede pasar es [256].\n",
    "* ranges : Este es nuevo rango. Normalmente está situado entre 0 y 256.\n",
    "\n",
    "En este histograma, podemos analizar como los elementos más blancos, los situados a la derecha del histograma, identificando más al pato y a la nubes. Y por otro lado tendría la parte más cercana al 0, que sería la izquierda de la gráfica, que vendría a identificar a los pixeles con un color más cercanos al negro puro.\n",
    "\n",
    "**Ejercicio:** Intenta identificar en una imagen en escala de grises (elegida a tu eleccion), los diferentes objetos que aparecen en la imagen a raiz del histograma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tip_01_01.py\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"dataset/examples/Holy-Grail.jpg\")\n",
    "cv2.imshow(\"dataset/examples/Holy-Grail.jpg\", img)\n",
    "cv2.waitKey(0)\n",
    "color = ('b', 'g', 'r')\n",
    "\n",
    "for i, c in enumerate(color):\n",
    "    hist = cv2.calcHist([img], [i], None, [256], [0, 256])\n",
    "    plt.plot(hist, color=c)\n",
    "    plt.xlim([0, 256])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecualización\n",
    "\n",
    "Es una transformación que pretende obtener para una imagen un histograma con una distribución uniforme. Es decir, que exista el mismo número de pixels para cada nivel de gris del histograma de una imagen monocroma.\n",
    "\n",
    "### cv2.equalizeHist(src[, dst]) \n",
    "* src: imagen a procesar, debe estar en escala de grises.\n",
    "\n",
    "Nos devuelve la imagen ecualizada a través de su histograma.\n",
    "\n",
    "**Ejercicio:** prueba a usar diferentes imágenes con diferentes condiciones de luz, ecualizarlas y comprobar los resultados obtenidos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tip_01_02_a.py\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "img = cv2.imread(\"dataset/examples/life-of-brian2.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imshow('Always look on the bright side of life', img)\n",
    "cv2.waitKey(0)\n",
    "hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n",
    "plt.plot(hist, color='gray')\n",
    "\n",
    "plt.xlabel('intensidad de iluminacion')\n",
    "plt.ylabel('cantidad de pixeles')\n",
    "plt.show()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliografía**\n",
    "\n",
    "https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_table_of_contents_histograms/py_table_of_contents_histograms.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ecualización adaptativa\n",
    "\n",
    "Hay veces que cuando la imagen dispone de una amplia gama de colores (escala de grises), no obtenemos grandes mejoras en el contraste. Esto es debido a que se usa un valor fijo de contraste en la ecualización.\n",
    "\n",
    "Para mejorar este apartado disponemos de funciones de ecualización de histograma adaptativo. Para ello, la imagen se divide en pequeños bloques llamados “tiles” (tileSize es 8×8 por defecto en OpenCV). Cada uno de estos bloques se ecualiza de manera independiente. Lo que supone un histograma de pequeñas zonas de la imagen, teniendo un contraste más adaptado, y no tan general como en el caso anterior, que para imágenes con muchos matices, donde tenderán a perderse la definición de los detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tip_01_02_b.py\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Ecualizacion de histogramas\n",
    "img = cv2.imread(\"dataset/examples/life-of-brian2.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "img = cv2.equalizeHist(img)\n",
    "\n",
    "cv2.imshow('Histogramas', img)\n",
    "cv2.waitKey(0)\n",
    "hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n",
    "plt.plot(hist, color='gray')\n",
    "\n",
    "plt.xlabel('intensidad de iluminacion')\n",
    "plt.ylabel('cantidad de pixeles')\n",
    "plt.show()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tip_01_02_c.py\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Ecualizacion de histograma adaptativo\n",
    "\n",
    "img = cv2.imread(\"dataset/examples/life-of-brian2.jpg\", cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imshow('Always look on the bright side of life', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "img = clahe.apply(img)\n",
    "\n",
    "hist = cv2.calcHist([img], [0], None, [256], [0, 256])\n",
    "plt.plot(hist, color='gray')\n",
    "cv2.imshow('Always look on the bright side of life', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "plt.xlabel('intensidad de iluminacion')\n",
    "plt.ylabel('cantidad de pixeles')\n",
    "plt.show()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tip_01_02_d.py\n",
    "import cv2\n",
    "\n",
    "# Ecualizacion de histograma adaptativo\n",
    "\n",
    "img = cv2.imread(\"dataset/examples/life-of-brian2.jpg\", 1)\n",
    "cv2.imshow(\"Always look on the bright side of life\", img)\n",
    "cv2.waitKey(0)\n",
    "# -----Converting image to LAB Color model-----------------------------------\n",
    "lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)\n",
    "cv2.imshow(\"lab\", lab)\n",
    "cv2.waitKey(0)\n",
    "# -----Splitting the LAB image to different channels-------------------------\n",
    "l, a, b = cv2.split(lab)\n",
    "cv2.imshow('l_channel', l)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('a_channel', a)\n",
    "cv2.waitKey(0)\n",
    "cv2.imshow('b_channel', b)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# -----Applying CLAHE to L-channel-------------------------------------------\n",
    "clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "cl = clahe.apply(l)\n",
    "cv2.imshow('CLAHE output', cl)\n",
    "cv2.waitKey(0)\n",
    "# -----Merge the CLAHE enhanced L-channel with the a and b channel-----------\n",
    "limg = cv2.merge((cl, a, b))\n",
    "cv2.imshow('limg', limg)\n",
    "cv2.waitKey(0)\n",
    "# -----Converting image from LAB Color model to RGB model--------------------\n",
    "final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "cv2.imshow('final', final)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliografía**\n",
    "\n",
    "https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_histograms/py_table_of_contents_histograms/py_table_of_contents_histograms.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operadores morfológicos\n",
    "\n",
    "La Morfología matemática ha demostrado ser una herramienta importante en el análisis de imágenes cuando la topología y la estructura geométrica de los objetos presentes en ellas son los parámetros claves para su caracterización. \n",
    "\n",
    "Es una técnica de procesamiento y análisis de imágenes relativamente joven que ha demostrado gran capacidad para solventar una amplia gama de problemas sobre imágenes binarias (en blanco y negro) o numéricas (en escala de grises o a color). \n",
    "\n",
    "* Las operaciones morfológicas simplifican imágenes y conservan las principales características de forma de los objetos. \n",
    "* Un sistema de operadores de este tipo y su composición, permite que las formas subyacentes sean identificadas y reconstruidas de forma Morfología óptima a partir de sus formas distorsionadas y ruidosas. \n",
    "* La morfología matemática se puede usar, entre otros, con los siguientes objetivos: \n",
    "    * Pre-procesamiento de imágenes (supresión de ruidos, simplificación de formas). \n",
    "    * Destacar la estructura de los objetos (extraer el esqueleto, detección de objetos, envolvente convexa, ampliación, reducción,...)\n",
    "\n",
    "Evidentemente esta técnica no puede solventar por sí sola todos los posibles problemas que se puede presentar en una aplicación de imágenes digitales, pero en los casos donde es útil, suele ser la opción más eficiente y de más fácil implementación.\n",
    "\n",
    "Existen dos operadores básicos Erosión y Dilatación. Y de su combinacio las variantes como apertura, cierre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Erosión\n",
    "El kernel se desliza a través de la imagen (como en la convolución 2D). Un píxel en la imagen original (ya sea 1 o 0) se considerará 1 solo si todos los píxeles del kernel son 1, de lo contrario se erosiona (se convierte en cero)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip_01_04_a.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"dataset/examples/erosion-dilatation.png\", 0)\n",
    "\n",
    "cv2.imshow('Paradigma', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "erosion = cv2.erode(img, kernel, iterations=1)\n",
    "\n",
    "cv2.imshow('Paradigma', erosion)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilatación\n",
    "Es justo lo opuesto a la erosión. Aquí, el píxel resultado es '1' si al menos un píxel del kernel es '1'. Por lo tanto, aumenta la región blanca en la imagen o aumenta el tamaño del objeto en primer plano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip_01_04_b.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"dataset/examples/erosion-dilatation.png\", 0)\n",
    "\n",
    "cv2.imshow('Paradigma', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "dilation = cv2.dilate(img, kernel, iterations=1)\n",
    "\n",
    "cv2.imshow('Paradigma', dilation)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apertura\n",
    "Apertura, es el proceso de aplicar una erosión, seguido de una dilatación. Es muy útil para eliminar el ruido, o pequeños elementos que ensucian una imagen.\n",
    "\n",
    "Para ello usamos la función: `cv2.morphologyEx()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip_01_04_c.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"dataset/examples/apertura.png\", 0)\n",
    "\n",
    "cv2.imshow('Paradigma', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "cv2.imshow('Paradigma', opening)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clausura\n",
    "Clausura es el proceso contrario a Apertura, seria un operación de dilatación seguida de una erosión. Es útil para cerrar pequeños agujeros dentro de los objetos de primer plano, o pequeños puntos negros en el objeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip_01_04_d.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"dataset/examples/clausura.png\", 0)\n",
    "\n",
    "cv2.imshow('Paradigma', img)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "cv2.imshow('Paradigma', closing)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentación Watershed\n",
    "Cualquier imagen en escala de grises se puede ver como una superficie topográfica donde una intensidad alta indica picos y colinas, mientras que intensidades bajas indican valles. En este algoritmo se empieza por llenar cada valle aislado (mínimos locales) con agua de diferentes colores (etiquetas). A medida que el agua sube, dependiendo de los picos (pendientes) cercanos, el agua de diferentes valles, obviamente con diferentes colores, comenzará a fusionarse. Para evitar esto, se construyen barreras en los lugares donde se une el agua. Luego, se continúa el trabajo de rellenar con agua y construir barreras hasta que todos los picos estén bajo el agua. Así, las barreras creadas en este proceso, no son más que la segmentación de la imagen. Esta es la “filosofía” detrás del algoritmo Watershed.\n",
    "\n",
    "Sin embargo, este enfoque da un resultado sobre-segmentado debido al ruido o a cualquier otra irregularidad en la imagen. Así que OpenCV implementó un algoritmo de cuenca hidrográfica basado en marcadores en el que se especifican cuáles son todos los puntos del valle que se fusionarán y cuáles no. Es una segmentación de imagen interactiva. Lo que hacemos es dar diferentes etiquetas a nuestro objeto. De este modo, tendremos que etiquetar la región que estamos seguros de que es el primer plano o el objeto en sí con un color (o intensidad), y debemos etiquetar la región de la que estamos seguros que es el fondo y no el objeto, con otro color. Finalmente la región de la que no estamos seguros de nada la debemos etiquetar con 0. Ese es nuestro marcador. Sólo después de este etiquetado aplicamos el algoritmo Watershed. Entonces nuestro marcador se actualizará con las etiquetas que dimos, y los límites de los objetos tendrán un valor de -1.\n",
    "\n",
    "A continuación, veremos un ejemplo sobre cómo usar la Transformación de distancia junto con el Watershed para segmentar objetos que se tocan mutuamente.\n",
    " \n",
    " \n",
    "Considere la imagen de las monedas a continuación, las monedas se tocan entre sí. Incluso si lo limitas, se tocarán entre sí.\n",
    "\n",
    "Comenzamos por encontrar una estimación aproximada de las monedas. Para eso, podemos usar la binarización de Otsu.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv2.imread('monedas.png')\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(gray,0,255,cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "```\n",
    "\n",
    "Ahora necesitamos eliminar cualquier pequeño ruido blanco en la imagen. Para eso podemos usar la apertura morfológica. Para eliminar cualquier agujero pequeño en el objeto, podemos usar el cierre morfológico. Por lo tanto, ahora sabemos con certeza que la región cercana al centro de los objetos está en primer plano y que la región más alejada del objeto es el fondo. Solo la región de la que no estamos seguros es la región límite de las monedas.\n",
    "\n",
    "Entonces necesitamos extraer el área de la cual estamos seguros que son monedas. La erosión elimina los píxeles del límite. Entonces, lo que quede, podemos estar seguros de que es una moneda. Eso funcionaría si los objetos no se tocaran entre sí. Pero como se están tocando entre sí, otra buena opción sería encontrar la distancia de transformación y aplicar un umbral adecuado. Luego tenemos que encontrar el área que estamos seguros de que no son monedas. Para eso, dilatamos el resultado. La dilatación aumenta el límite del objeto al fondo. De esta forma, podemos asegurarnos de que cualquier región en el fondo en el resultado sea realmente un fondo, ya que la región límite se elimina.\n",
    "\n",
    "Las regiones restantes son aquellas de las que no tenemos idea de si son monedas o fondo. El algoritmo de Watershed debería ser capaz de discriminar entre monedas y fondo en estas regiones conflictivas. Estas regiones corresponden al área alrededor de los límites de las monedas donde se cruzan el primer plano y el fondo (o incluso se encuentran dos monedas diferentes). Tales regiones delimitantes son las fronteras. Se puede obtener restando el área de la figura de la izquierda del área de la figura de la derecha.\n",
    "\n",
    "```python\n",
    "# Usaremos el operador morfológico de Apertura como vimos en el reto anterior, para la eliminación del ruido de la imagen.\n",
    "kernel = np.ones((3,3),np.uint8)\n",
    "opening = cv2.morphologyEx(thresh,cv2.MORPH_OPEN,kernel, iterations = 2) \n",
    " \n",
    "# Encuentra el área del fondo\n",
    "sure_bg = cv2.dilate(opening,kernel,iterations=3)\n",
    " \n",
    "# Encuentra el área del primer plano\n",
    "dist_transform = cv2.distanceTransform(opening,cv2.DIST_L2,5)\n",
    "ret, sure_fg = cv2.threshold(dist_transform,0.7*dist_transform.max(),255,0)\n",
    " \n",
    "# Encuentra la región desconocida (bordes)\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(sure_bg,sure_fg)\n",
    "```\n",
    "\n",
    "Vea el resultado en la imagen a la que se ha aplicado un umbral; se obtienen algunas regiones de monedas de las cuales estamos seguros de las monedas y que ahora además están separadas.\n",
    "En algunos casos, puede interesarle solo la segmentación del primer plano, y no en separar los objetos que se tocan mutuamente. En ese caso, no necesita usar la transformación de distancia, basta con la erosión. La erosión es solo otro método para extraer el área de primer plano.\n",
    "\n",
    "Ahora sabemos con certeza cuáles son las regiones de las monedas, que es parte del fondo y el resto. Ahora creamos marcador (es un arreglo del mismo tamaño que el de la imagen original, pero con el tipo de datos int32) y etiquetamos las regiones dentro de él. Las regiones que sabemos con certeza (ya sea en primer plano o en segundo plano) están etiquetadas con números enteros positivos, pero enteros diferentes, y el área que no sabemos con certeza simplemente queda en cero. Para esto usamos **cv2.connectedComponents()**. Con esta función etiquetamos el fondo de la imagen con 0, y el resto de los objetos quedan etiquetados con números enteros a partir de 1.\n",
    "Pero sabemos que si el fondo está marcado con 0, el algoritmo de Watershed lo considerará como un área desconocida. Por tanto, queremos marcarlo con un número entero diferente. En cambio, marcaremos la región desconocida, definida como unknown, con 0.\n",
    "\n",
    "```python\n",
    "# Etiquetado\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "# Adiciona 1 a todas las etiquetas para asegurra que el fondo sea 1 en lugar de cero\n",
    "markers = markers+1\n",
    "# Ahora se marca la región desconocida con ceros\n",
    "markers[unknown==255] = 0\n",
    "```\n",
    "\n",
    "Vea el resultado que se muestra en el mapa de colores JET. La región azul oscura muestra la región desconocida. Las monedas están coloreadas con diferentes valores. El área restante, que es de fondo seguro, se muestra en azul claro en comparación con la región desconocida.\n",
    "\n",
    "Ahora nuestro marcador está listo. Es hora de dar el paso final, aplicar el algoritmo de Watershed. Al hacer esto la imagen del marcador será modificada y la región límite será etiquetada con -1.\n",
    "\n",
    "```python\n",
    "markers = cv2.watershed(img,markers)\n",
    "img[markers == -1] = [255,0,0]\n",
    "```\n",
    "\n",
    "Vea el resultado a continuación. Aunque en general, el algoritmo encuentra muy bien las fronteras de las monedas, en algunos casos, en las regiones de contacto entre dos monedas, el algoritmo falla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip_01_05.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread(\"dataset/examples/water_coins.jpg\", 1)\n",
    "cv2.imshow('Coins', img)\n",
    "cv2.waitKey(0)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "cv2.imshow('Watershed1', thresh)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# noise removal\n",
    "kernel = np.ones((3, 3), np.uint8)\n",
    "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "# sure background area\n",
    "sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "# Finding sure foreground area\n",
    "dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "ret, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "\n",
    "# Finding unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "\n",
    "cv2.imshow('Watershed2', sure_fg)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.imshow('Watershed3', unknown)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Marker labelling\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers + 1\n",
    "\n",
    "# Now, mark the region of unknown with zero\n",
    "markers[unknown == 255] = 0\n",
    "\n",
    "markers = cv2.watershed(img, markers)\n",
    "img[markers == -1] = [255, 0, 0]\n",
    "\n",
    "cv2.imshow('Watershed4', img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliografía**\n",
    "\n",
    "Para este reto hemos utilizado el articulo https://www.aprenderpython.net/segmentacion-imagenes-algoritmo-watershed/, por su facil explicacion de este algortimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detección de contornos\n",
    "\n",
    "Los contornos se pueden explicar simplemente como una curva que une todos los puntos contiguos (a lo largo de un límite), teniendo el mismo color o intensidad. Los contornos son una herramienta útil para el análisis de formas y la detección y el reconocimiento de objetos.\n",
    "* Para una mayor precisión, use imágenes binarias. Antes de iniciar la detección de contornos es conveniente la aplicación de umbralización.\n",
    "* La función findContours modifica la imagen de origen. Entonces, si quiere imagen de origen incluso después de encontrar contornos, ya la almacena en algunas otras variables.\n",
    "* En OpenCV, encontrar contornos es como encontrar un objeto blanco desde un fondo negro. Así que recuerde, el objeto que se debe encontrar debe ser blanco y el fondo debe ser negro.\n",
    "\n",
    "Como encontramos los contornos en una imagen binaria\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "im = cv2.imread('test.jpg')\n",
    "imgray = cv2.cvtColor(im,cv2.COLOR_BGR2GRAY)\n",
    "ret,thresh = cv2.threshold(imgray,127,255,0)\n",
    "image, contours, hierarchy = cv2.findContours(thresh,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "```\n",
    "\n",
    "Para ello usaremos la función cv2.findContours (). Esta función recibe 3 argumentos, el primero es la imagen de origen, el segundo es el modo de recuperación de contorno, el tercero es el método de aproximación de contorno. Y devuelve la imagen, los contornos y la jerarquía de los mismos. Los contornos es una lista de Python de todos los contornos de la imagen. Cada contorno individual es una matriz Numpy de coordenadas (x, y) de puntos límite del objeto.\n",
    "\n",
    "\n",
    "## Dibujar contornos\n",
    "Una vez que hemos identificado las coordenadas de los contornos, es interesante dibujarlos sobre la imagen, principalmente para saber si se han detectado de manera correcta.\n",
    "\n",
    "Para dibujar los contornos, se usa la función cv2.drawContours. También se puede usar para dibujar cualquier forma siempre que tenga sus puntos de límite. Su primer argumento es la imagen fuente, el segundo argumento son los contornos que deben pasarse como una lista de Python, el tercer argumento es el índice de contornos (útil al dibujar un contorno en concreto, para dibujar todos los contornos, pasar -1) y los argumentos restantes son el color y grosor de la línea de dibujo.\n",
    "\n",
    "Código para dibujar todos los contornos de una imagen:\n",
    "```python\n",
    "img = cv2.drawContours(img, contours, -1, (0,255,0), 3)\n",
    "```\n",
    "\n",
    "Código para dibujar un contorno en concreto. En este caso el contorno situado en la posición 4 del array de contornos:\n",
    "```python\n",
    "img = cv2.drawContours(img, contours, 3, (0,255,0), 3)\n",
    "```\n",
    "\n",
    "**Ejercicios adicionales**\n",
    "\n",
    "En el ejemplo de a continuacion, podeis ver el desarrollo para la detección de contornos, para el caso de objetos de color rojo. (Ver en el notebook).\n",
    "\n",
    "**Ejercicio:** detecta los contornos a imagen de tu elección y analiza que los resultados obtenidos son similares al ejemplo anterior. Pudiendo identificarse claramente los contornos de los objetos cuyo color está dentro del rango de color establecido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip_01_06.py\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('dataset/examples/contours.png')\n",
    "\n",
    "cv2.imshow(\"Show\", img)\n",
    "cv2.waitKey()\n",
    "\n",
    "hsv_img = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "# lower mask (0-10)\n",
    "lower_red = np.array([0, 50, 50])\n",
    "upper_red = np.array([10, 255, 255])\n",
    "mask0 = cv2.inRange(hsv_img, lower_red, upper_red)\n",
    "\n",
    "# upper mask (170-180)\n",
    "lower_red = np.array([170, 130, 130])\n",
    "upper_red = np.array([180, 255, 255])\n",
    "mask1 = cv2.inRange(hsv_img, lower_red, upper_red)\n",
    "\n",
    "# join my masks\n",
    "mask = mask0 + mask1\n",
    "\n",
    "# set my output img to zero everywhere except my mask\n",
    "output_img = img.copy()\n",
    "output_img[np.where(mask == 0)] = 0\n",
    "\n",
    "# or your HSV image, which I *believe* is what you want\n",
    "output_hsv = hsv_img.copy()\n",
    "output_hsv[np.where(mask == 0)] = 0\n",
    "\n",
    "ret, thresh = cv2.threshold(output_hsv, 127, 255, 0)\n",
    "cv2.imshow(\"Show\", thresh)\n",
    "cv2.waitKey()\n",
    "gray = cv2.cvtColor(thresh, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "image, contours, hierarchy = cv2.findContours(gray, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "cv2.drawContours(img, contours, -1, (0, 255, 0), 3)\n",
    "\n",
    "cv2.imshow(\"Show\", img)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliografía**\n",
    "\n",
    "http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_contours/py_contours_begin/py_contours_begin.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuara...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
